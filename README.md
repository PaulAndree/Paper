
<h3 align="center">Adaptive Learning Assessment Enhanced by Language Models </h3>

Abstract.

We present an educational framework that automates the
generation and assessments of questionnaires without domain-specific
and language constraints by leveraging Natural Language Processing
(NLP) and advanced Language Models (LMs) to internalize Bloom’s
Taxonomy. Our framework categorizes questions into three distinct difficulty
levels, addressing the core challenge of transferring the structured
knowledge of cognitive and learning levels into LMs. We hypothesize that
these difficulty levels can be effectively represented by grouping Bloom’s
categories, facilitating the model’s understanding and generation of appropriate
questions. To test this hypothesis, we generated multiple-choice
and open-ended questions, evaluating their syntactic construction and
semantic integrity. Our experiments demonstrate a robust alignment between
the proposed difficulty levels and the generated questions compared
to our baseline. The framework consistently produces questions
that are semantically accurate, syntactically precise, and contextually
relevant. Additionally, the automatic evaluation model for open-ended
questions provides accurate scores and feedback on student responses,
further supporting effective self-assessment. These findings underscore
the potential of our approach to enhance the learning process by effectively
transferring the structured knowledge of Bloom’s Taxonomy into
LMs. This enables the generation of high-quality, difficulty-leveled questions
without domain-specific constraints, thus promoting efficient and
adaptive learning experiences.
